# -*- coding: utf-8 -*-
"""Data Marketing Labs - Gemini Text Generation - SGE - For Students

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14lLmgV2016aaljNYENEdCi_PWGQ-5ZMJ

# DATA MARKETING LABS : Search Generative Experience

- FR : https://www.datamarketinglabs.com/formation-sge-search-generative-experience

- EN : https://www.datamarketinglabs.com/

Objectives : Extract all concepts from LLM with Palm2 and OpenAI

Author : Vincent Terrasi

Licence : CC BY-SA (Creative Commons Attribution-ShareAlike)

**Update 2025 : Bard is replaced by Gemini**

## Initialisation du projet
"""

!pip install google-cloud-aiplatform
!pip install fuzzywuzzy
!pip install retrying

PROJECT = "focal-column-368023" # @param {type:"string"}
LOCATION = "us-central1" # @param {type:"string"}

"""## Text Generation"""

from google.colab import auth as google_auth
google_auth.authenticate_user()

bard_call_counter = 0

import vertexai
from vertexai.preview.generative_models import GenerativeModel, Part
import vertexai.preview.generative_models as generative_models
import pandas as pd

from collections import Counter
import concurrent.futures
from retrying import retry
from fuzzywuzzy import fuzz, process
from tenacity import retry, wait_random_exponential, stop_after_attempt

import time
import re
import unicodedata

##NEW
import ast
##

#TYPE = 'tech blog post' # @param {type:"string"}
#TITLE = 'The PC Tower: A Guide for Gamers and High-Performance Users' # @param {type:"string"}
#GOAL = 'Explaining the benefits and structure of a PC Tower' # @param {type:"string"}
#KEYWORDS = 'PC tower, gaming PCs, high-performance PCs, components, cooling' # @param {type:"string"}
#CONTEXT = 'A step-by-step guide to understanding a PC tower' # @param {type:"string"}

TYPE = 'Guide/Review' # @param {type:"string"}
TITLE = 'Mastering the Art of PC Case Selection: Features, Benefits, and Expert Recommendations' # @param {type:"string"}
GOAL = 'To empower the reader with knowledge about various PC case features and guide them in making the best selection for their needs.' # @param {type:"string"}
KEYWORDS = 'PC case, cooling fans, ports, expansion slots, metal PC case, plastic PC case, airflow, cable management, motherboard compatibility, design aesthetics' # @param {type:"string"}
CONTEXT = 'Personal experience of building PCs and comprehensive research on the latest PC case trends, features, and user reviews.' # @param {type:"string"}


user_prompt = '''
Write a 800 word in-depth LONG detailed {TYPE}.
Use H1, H2 u li headlines.
The title is {TITLE}.
Share your experience and explain {GOAL} in a step-by-step format.
Use keywords : {KEYWORDS}.
Use the {CONTEXT} to get your information.
'''

bard_call_counter = 0

# EXPERIENCE
system_prompt = "Your name is Miss Galadriel. You are creative reasearcher and writer. You write very intriging blog in FIRST PERSON about a given topic. "
user_prompt = user_prompt.format(TYPE=TYPE, TITLE=TITLE, GOAL=GOAL, KEYWORDS=KEYWORDS, CONTEXT=CONTEXT)


# Define a function that determines whether to retry an exception or not.
# In this case, it always retries.
def retry_if_exception(exception):
    return True


@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def bard_call(system_prompt, user_prompt):
    """
    Appelle le modèle gemini et traite la réponse pour éviter les problèmes d'encodage.
    """
    global bard_call_counter
    bard_call_counter += 1
    print(f"Gemini called {bard_call_counter} times.")

    vertexai.init(project=PROJECT, location=LOCATION)

    model = GenerativeModel(
        "gemini-1.5-pro-001",
        system_instruction=[system_prompt]
    )

    responses = model.generate_content(
        [user_prompt],
        generation_config={
            "max_output_tokens": 2000,
            "temperature": 0.5,
            "top_p": 1.0,
            "top_k": 40
        },
        safety_settings={
            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
        },
        stream=False,
    )

    # Conversion des réponses en chaîne brute
    data = str(responses)

    # Nettoyage des caractères mal encodés
    cleaned_data = clean_text(data)

    # Extraction du texte utile
    text_parts = re.findall(r'text: "(.*?)"', cleaned_data, re.DOTALL)
    extracted_text = "\n\n".join(text_parts)

    # Remplacer les séquences d'échappement littérales par leurs équivalents réels
    extracted_text = extracted_text.replace('\\n', '\n')

    # Préparation pour Markdown - Assurer que les sauts de ligne sont préservés
    # Ajouter deux espaces à la fin de chaque ligne pour forcer les sauts de ligne en Markdown
    lines = extracted_text.split('\n')
    markdown_lines = []
    for line in lines:
        if line.strip():  # Si la ligne n'est pas vide
            markdown_lines.append(line + '  ')
        else:
            markdown_lines.append(line)

    extracted_text = '\n'.join(markdown_lines)

    return extracted_text

def decode_to_utf8(data):
    """
    Essaie de décoder les données en UTF-8 ou corrige les erreurs d'encodage.
    """
    try:
        # Décodage direct en UTF-8
        return data.decode('utf-8')
    except (UnicodeDecodeError, AttributeError):
        # Si échec, essayer latin1
        return data.decode('latin1').encode('utf-8').decode('utf-8')


def normalize_text(text):
    """
    Normalise le texte pour corriger les caractères spéciaux mal affichés.
    """
    return unicodedata.normalize('NFKC', text)


def clean_text(data):
    """
    Nettoie et normalise le texte pour éviter les caractères mal encodés.
    """
    # Étape 1 : Décodage des données
    if not isinstance(data, str):  # Si les données sont en bytes
        try:
            decoded_data = decode_to_utf8(data)
        except Exception as e:
            print(f"Error during decoding: {e}")
            decoded_data = data.decode('latin1', errors='replace')
    else:
        decoded_data = data

    # Étape 2 : Normalisation Unicode
    normalized_data = normalize_text(decoded_data)

    return normalized_data

generated_text = bard_call(system_prompt, user_prompt)

from IPython import display

display.Markdown(data=generated_text)

user_prompt1 = "To the best of your knowledge, is the following article factually accurate on all accounts? If not, please list the inaccuracies and provide the correct facts instead.\n\nHere is the article:\n\n" + generated_text
generated_text1 = bard_call(system_prompt, user_prompt1)
display.Markdown(data=generated_text1)

user_prompt2="Are there any sentences or paragraphs in the following article that simply summarize what was said earlier in the article:\n\n" + generated_text
generated_text2 = bard_call(system_prompt, user_prompt2)
display.Markdown(data=generated_text2)

user_prompt3="Is there any unnecessary “fluff” in the following article:\n\n" + generated_text
generated_text3 = bard_call(system_prompt, user_prompt3)
display.Markdown(data=generated_text3)

user_prompt4="Please break up the paragraphs in the following article. No paragraph should have more than ten sentences. Ideally, each paragraph should be just one or max 5 sentences in length. Keep all headings.:\n\n" + generated_text
generated_text4 = bard_call(system_prompt, user_prompt4)
display.Markdown(data=generated_text4)