# Comment extraire toutes les notions apprises par l’IA

Cette présentation technique expose une méthodologie pour automatiser la collecte et l'analyse de connaissances via des agents intelligents intégrés dans un environnement Google Colab. Le processus repose sur l'utilisation de l'API Google Custom Search pour extraire des données web, complétée par l'outil Trafilatura afin de cibler précisément le contenu textuel essentiel des pages. L'innovation majeure réside dans l'architecture hybride associant deux modèles de langage : GPT-3.5 Turbo est sollicité pour sa précision dans l'extraction d'entités nommées, tandis que PaLM 2 (Bison) génère les réponses et les questions structurées. Pour optimiser l'efficacité du système, l'auteur utilise le multi-threading afin d'exécuter les requêtes en parallèle, offrant ainsi un cadre flexible que les développeurs peuvent adapter et modifier selon leurs besoins spécifiques.

Continuons avec une vidéo qui sera très technique. Donc si vous êtes allergique au code, vous invite à passer cette vidéo. Sinon, si vous êtes développeur, on va vraiment rentrer dans le détail du fonctionnement de cet algorithme et ceci va vous permettre de l'adapter à vos besoins. Nous revoici dans le Google collab. Donc ce qu'on va faire, c'est qu'on va passer cette phase de setup qu'on a vu dans la vidéo précédente, la phase de réglage pour aller tout de suite ici dans la Google Search API. Donc on va cliquer ici sur afficher le code et Et comme vous voyez donc on retrouve le code qu'on utilisé pour les agents. Donc là on utilise l'API Custom Search de Google afin de récupérer les premiers résultats. J'ai laissé deux fonctions pour faire de on va dire du résumé de texte extractif et abstractif. Donc extractif vous pouvez extraire les les phrases principales et l'abstractif c'est il va paraphraser autour de du résumé qui sera généré. Mais là dans la démo, moi ce j'utilise principalement C'est le snipet. Donc c'est tout simplement le le résumé qui est proposé par l'APAPI de Google. Donc l'API de Google nous retourne un titre, un lien et un résumé. Et après à l'occasion, vous pouvez décider de nourrir donc dans les instructions des contenus qui sont récupérés. Alors les contenus, comment je les récupère ? Ils sont récupérés avec Trafilatura qui est sûrement la meilleure tech à l'heure actuelle pour extraire le contenu principal d'une page. Bien sûr ici vous avez vu Vous pouvez modifier la limite. Donc par exemple, si je veux que cinq résultats, je vais venir modifier ici. Vous pouvez aussi choisir votre langue. Donc par défaut, c'est en anglais mais vous pouvez basculer sur du français. Et après, il y a des réglages très très fins qui peuvent être fait avec la Google Search API. Alors, on peut sélectionner le le country code, la langue mais même la région. Donc quand quand c'est quand c'est pertinent, bien sûr. Donc ça c'était pas la partie la plus compliquée. La partie la plus compliquée, elle se trouve ici, c'est l'utilisation à la fois de Palme 2 et d'open vous allez comprendre pourquoi j'ai mixé les deux et pourquoi j'ai pas tout fait avec Palme 2\. Quand on ouvre ce code, donc on retrouve le préprom qu'on a utilisé précédemment pour simuler Google Bar. Ici on trouve un appel à GPT 3.5 Turbo. Vous allez voir les performances vont être très importants. Donc ici nous avons la fonction extract entity. Donc elle porte très bien son nom. Elle va venir extraire toutes les les entités importantes dans un texte. On aurait pu utiliser du machine learning avec des librairies comme Space, mais je trouve que Open EI est beaucoup plus performant pour le faire et si vous souhaitez des choses spécifiques, vous allez pouvoir le le mentionner ici. Donc là, je lui ai dit que je voulais extraire des entités nommées ou des expressions qui sont reliées à un domaine. On pourrait tout simplement imaginer qu'on souhaite extraire seulement des marques ou seulement des noms de personnes. Donc c'est pour ça que j'ai utilisé Open EI. Donc donc là, vous voyez, on récupère la réponse. Donc j'ai mis des prints mais vous pouvez les enlever si si vous le souhaitez. Après bien sûr l'appel à bard. Donc on voit bien que c'est Palme 2\. Ça on l'a déjà vu. On utilise l'IA qui s'appelle bison. Alors dans un futur proche il y aura forcément des bisons 002 003\. Faud faudra sûrement mettre à jour. La température faut la laisser à 0,5. Le nombre de tokens. J'évite de mettre trop de tokens pour la simple et bonne raison que déjà en entrée, on a Vous avez vu, on a le préprom de barde plus euh la query et plus les éléments euh extraits. Et enfin, on a un code donc là forcément on utilise Bard qui va demander à Bard de générer des questions autour d'un sujet. Donc là, vous voyez, c'est très simple à faire. Alors, de manière générale, le code de barde est très réduit parce que c'est une technologie Google et tout se fait dans Google collab. Donc, une fois que vous êtes connecté ici avec Google H Authentificate User, allez limite, vous vous donnez une phrase et vous attendez les résultats. Et après dans les informations pertinente sur ce code, bah ça c'est le cœur de la fonction avec Bard. C'estàd que là on va prendre chacune des des queries et pour gagner du temps, en fait on fait un trade et on exécute en parallèle l'intégralité des des appels parce que sinon ça peut durer bien trop longtemps. Et ce qu'on fait en fait c'est que à chaque fois on va on va extraire les les entités dans les dans les textes qui sont générés et la fonction analyse, on la verra plus tard comme la fonction générer un rapport. Donc voilà, vous savez tout et maintenant vous comprenez l'importance de de querybard. En fait, c'est vraiment la la fonction qui déclenche l'intégralité des fonctions. Donc maintenant, je vous invite à faire une pause et à lire cette fonction à tête reposée. Surtout n'hésitez pas à venir la modifier. Voyez ? Et ici, j'ai récupéré tous les snipets que je mets dans le prompt, mais vous pouvez injecter les contenus de votre choix. C'était vraiment l'idée de ce cours, c'est de donner une méthodologie, de l'expliquer et après de permettre de l'adapter à vos usages. Donc bonne lecture et j'ai envie de dire même bonne modification et rendez-vous sur la vidéo suivante.

L'automatisation de contenu, telle que présentée dans les sources, repose sur l'utilisation de scripts et d'IA pour optimiser la phase cruciale de recherche sémantique et de planification éditoriale. Au lieu de rédiger manuellement, cette approche utilise des algorithmes pour identifier les concepts clés qui garantissent une meilleure indexation.  
Voici les piliers de cette automatisation :  
1\. Simulation de l'intention de recherche (SGE)  
L'automatisation commence par simuler le comportement des moteurs de recherche modernes comme Google SGE. Le script ne se contente pas d'une seule requête ; il automatise la génération de multiples questions périphériques autour d'un sujet (caractéristiques, avantages, inconvénients, etc.) pour explorer toute la profondeur sémantique du domaine. Plus le nombre de requêtes générées est élevé, plus l'information extraite de l'IA est riche.  
2\. Extraction automatisée de données (Web & IA)  
L'automatisation s'appuie sur une synergie entre les données en temps réel et les connaissances pré-entraînées des modèles de langage :

* Données de recherche : L'algorithme utilise l'API Google Custom Search pour récupérer automatiquement les titres, les liens et les "snippets" (extraits) des meilleurs résultats actuels.  
* Nettoyage de contenu : Des outils comme Trafilatura sont intégrés pour extraire proprement le contenu principal des pages web, éliminant le "bruit" inutile pour l'analyse.  
* Synthèse : Le système peut automatiser le résumé de texte, soit de manière extractive (phrases clés), soit de manière abstractive (paraphrase).

3\. Identification des « entités remarquables »  
Le cœur de cette automatisation est l'extraction d'entités nommées ou de concepts spécifiques.

* Le choix technologique : Bien que des outils de machine learning classiques existent, l'utilisation d'OpenAI (GPT-3.5 Turbo) est privilégiée pour sa performance supérieure dans l'identification d'expressions propres à un domaine spécifique.  
* Analyse de fréquence : Le script regroupe automatiquement les entités les plus fréquentes. L'idée est que si vous utilisez ces concepts (ex: "ventilation", "gestion de câbles" pour un PC), votre contenu s'indexera beaucoup plus facilement car il correspond aux connaissances déjà établies par les modèles de langage.

4\. Flux de travail technique et gain de temps  
Pour rendre ce processus viable à grande échelle, plusieurs techniques d'optimisation sont employées :

* Traitement en parallèle (Threading) : Pour éviter des temps d'attente trop longs, l'algorithme exécute tous les appels API (Google, OpenAI, PaLM 2\) simultanément.  
* Export automatisé : Les résultats (concepts, entités, analyses) sont automatiquement compilés dans des fichiers Excel ou des rapports, permettant au rédacteur de disposer immédiatement d'une feuille de route sémantique.

En résumé, l'automatisation ne remplace pas nécessairement la plume, mais elle industrialise la découverte du vocabulaire stratégique en fusionnant les connaissances internes des IA avec les meilleurs résultats de recherche de Google  
